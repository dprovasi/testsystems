{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit import Chem\n",
    "import networkx as nx\n",
    "from karateclub import Graph2Vec\n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdChemReactions\n",
    "from rdkit.Chem.EnumerateStereoisomers import EnumerateStereoisomers, StereoEnumerationOptions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mol_to_nx(mol):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        G.add_node(atom.GetIdx(),\n",
    "                   atomic_num=atom.GetAtomicNum(),\n",
    "                   is_aromatic=atom.GetIsAromatic(),\n",
    "                   atom_symbol=atom.GetSymbol())\n",
    "        \n",
    "    for bond in mol.GetBonds():\n",
    "        G.add_edge(bond.GetBeginAtomIdx(),\n",
    "                   bond.GetEndAtomIdx(),\n",
    "                   bond_type=bond.GetBondType())\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the CSV file\n",
    "csv_path = \"/Users/oliviacullen/Downloads/chembl30_kor_inhibitors.csv\"\n",
    "\n",
    "# Import CSV as DataFrame\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Graph Embeddings ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> read the data file ... \n",
      ">>> data shape =  (2056, 3)\n",
      ">>> data columns =  Index(['Name', 'SMILES', 'KOR Inhibitor'], dtype='object') \n",
      "\n",
      "              Name                                             SMILES  \\\n",
      "0         CHEMBL10  C[S+]([O-])c1ccc(-c2nc(-c3ccc(F)cc3)c(-c3ccncc...   \n",
      "1       CHEMBL1006                                  NCCCNCCSP(=O)(O)O   \n",
      "2       CHEMBL1009                     N[C@@H](Cc1ccc(O)c(O)c1)C(=O)O   \n",
      "3        CHEMBL101              CCCCC1C(=O)N(c2ccccc2)N(c2ccccc2)C1=O   \n",
      "4     CHEMBL101168                                     Nn1nnc2ccccc21   \n",
      "...            ...                                                ...   \n",
      "2051   CHEMBL98350              O=c1cc(N2CCOCC2)oc2c(-c3ccccc3)cccc12   \n",
      "2052     CHEMBL989  CC1(C)O[C@@H]2C[C@H]3[C@@H]4C[C@H](F)C5=CC(=O)...   \n",
      "2053     CHEMBL990             CN(Cc1ccc(C(C)(C)C)cc1)Cc1cccc2ccccc12   \n",
      "2054     CHEMBL991          Cc1cn([C@H]2C=C[C@@H](CO)O2)c(=O)[nH]c1=O   \n",
      "2055     CHEMBL998       CCOC(=O)N1CCC(=C2c3ccc(Cl)cc3CCc3cccnc32)CC1   \n",
      "\n",
      "      KOR Inhibitor  \n",
      "0                 0  \n",
      "1                 0  \n",
      "2                 0  \n",
      "3                 0  \n",
      "4                 0  \n",
      "...             ...  \n",
      "2051              0  \n",
      "2052              0  \n",
      "2053              0  \n",
      "2054              0  \n",
      "2055              0  \n",
      "\n",
      "[2056 rows x 3 columns]\n",
      "\n",
      ">>> create mol from smiles ... \n",
      ">>> create nx from mol ... \n",
      ">>> create graph embedding ... \n",
      ">>> df_graph2vec shape =  (2056, 128)\n",
      "           0         1         2         3         4         5         6    \\\n",
      "0     0.004480 -0.061732 -0.057173  0.037997 -0.017506 -0.014873  0.018292   \n",
      "1    -0.000859 -0.091228 -0.094152  0.063713 -0.010031 -0.020501  0.031428   \n",
      "2     0.003385 -0.081054 -0.081786  0.059659 -0.004941 -0.019902  0.028712   \n",
      "3     0.002821 -0.095135 -0.085900  0.052650 -0.018903 -0.016420  0.031420   \n",
      "4     0.003961 -0.064758 -0.061340  0.038367 -0.007127 -0.010496  0.022100   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "2051 -0.005647 -0.067397 -0.052987  0.044808 -0.013138 -0.006543  0.022581   \n",
      "2052 -0.007954 -0.150285 -0.130791  0.094326 -0.029354 -0.021147  0.050257   \n",
      "2053  0.001659 -0.084610 -0.070297  0.048647 -0.019765 -0.013761  0.024491   \n",
      "2054 -0.002152 -0.091190 -0.086072  0.053855 -0.017756 -0.020536  0.027841   \n",
      "2055  0.005276 -0.073387 -0.064265  0.051026 -0.015652 -0.010415  0.023805   \n",
      "\n",
      "           7         8         9    ...       118       119       120  \\\n",
      "0    -0.021518  0.038300 -0.025650  ... -0.045208 -0.022042 -0.010563   \n",
      "1    -0.036704  0.055098 -0.026959  ... -0.071801 -0.020171 -0.013929   \n",
      "2    -0.035559  0.045305 -0.021106  ... -0.052676 -0.015430 -0.021859   \n",
      "3    -0.040505  0.049946 -0.025012  ... -0.059437 -0.025257 -0.016839   \n",
      "4    -0.028344  0.042193 -0.022380  ... -0.034967 -0.015937 -0.015500   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "2051 -0.025548  0.047741 -0.020851  ... -0.051453 -0.013726 -0.006028   \n",
      "2052 -0.062079  0.093635 -0.038259  ... -0.107954 -0.036403 -0.027394   \n",
      "2053 -0.023823  0.055743 -0.017055  ... -0.066183 -0.017615 -0.021511   \n",
      "2054 -0.027281  0.051256 -0.023770  ... -0.058530 -0.025986 -0.014319   \n",
      "2055 -0.034803  0.050542 -0.015381  ... -0.053733 -0.024439 -0.009749   \n",
      "\n",
      "           121       122       123       124       125       126       127  \n",
      "0    -0.002725  0.106144 -0.007223 -0.052538  0.023361  0.051870  0.018177  \n",
      "1    -0.000787  0.155281 -0.018314 -0.081188  0.023515  0.075843  0.018628  \n",
      "2    -0.011610  0.117173 -0.021708 -0.054430  0.017070  0.049633  0.022726  \n",
      "3    -0.006279  0.130318 -0.025778 -0.067515  0.026758  0.063136  0.017185  \n",
      "4    -0.007897  0.087009 -0.005791 -0.038754  0.015071  0.038128  0.006204  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "2051 -0.011355  0.101194 -0.008019 -0.045860  0.017996  0.051653  0.012495  \n",
      "2052 -0.006179  0.225583 -0.030394 -0.106184  0.047381  0.101541  0.030368  \n",
      "2053 -0.002616  0.127602 -0.025430 -0.060147  0.027544  0.053534  0.023631  \n",
      "2054 -0.013689  0.133996 -0.022024 -0.058005  0.023292  0.064017  0.010996  \n",
      "2055 -0.003276  0.123010 -0.011964 -0.060118  0.023233  0.059447  0.010177  \n",
      "\n",
      "[2056 rows x 128 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\">>> read the data file ... \")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\">>> data shape = \", df.shape)\n",
    "print(\">>> data columns = \", df.columns, \"\\n\")\n",
    "print(df)\n",
    "print()\n",
    "\n",
    "print(\">>> create mol from smiles ... \")\n",
    "df['mol'] = df['SMILES'].apply(lambda x: Chem.MolFromSmiles(x)) \n",
    "\n",
    "print(\">>> create nx from mol ... \")\n",
    "df['graph'] = df['mol'].apply(lambda x: mol_to_nx(x))\n",
    "\n",
    "print(\">>> create graph embedding ... \")\n",
    "model = Graph2Vec()\n",
    "model.fit(df['graph'])\n",
    "df_graph2vec = model.get_embedding()\n",
    "\n",
    "df_graph2vec = pd.DataFrame(df_graph2vec)\n",
    "print(\">>> df_graph2vec shape = \", df_graph2vec.shape)\n",
    "print(df_graph2vec)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df and df_graph2vec are your dataframes\n",
    "\n",
    "# Create an empty list to store the appended rows\n",
    "appended_rows = []\n",
    "\n",
    "# Iterate over the rows of both dataframes\n",
    "for index, row in df.iterrows():\n",
    "    # Get the corresponding row from df2 as a list\n",
    "    row_list = list(df_graph2vec.iloc[index])\n",
    "    \n",
    "    # Append the row list to the current row of df1\n",
    "    appended_row = row.append(pd.Series(row_list))\n",
    "    \n",
    "    # Append the combined row to the list\n",
    "    appended_rows.append(appended_row)\n",
    "\n",
    "# Create a new dataframe from the appended rows list\n",
    "df_combined = pd.DataFrame(appended_rows)\n",
    "\n",
    "# Reset the index of the combined dataframe if needed\n",
    "df_combined.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a dataframe with all the LB properties ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_num_amide_bonds(molecule):\n",
    "    # Define the amide bond SMARTS pattern\n",
    "    amide_bond_smarts = Chem.MolFromSmarts(\"[*;$(C(=O)N);*]\")\n",
    "    \n",
    "    # Use the SMARTS pattern to match amide bonds in the molecule\n",
    "    amide_bonds = molecule.GetSubstructMatches(amide_bond_smarts)\n",
    "    \n",
    "    # Return the number of amide bonds\n",
    "    return len(amide_bonds)\n",
    "\n",
    "def calculate_num_heterocycles(molecule):\n",
    "    # Get the number of SSSR in the molecule\n",
    "    num_sssr = Chem.GetSSSR(molecule)\n",
    "    \n",
    "    # Count the number of rings that contain heteroatoms\n",
    "    num_heterocycles = 0\n",
    "    for ring in molecule.GetRingInfo().AtomRings():\n",
    "        has_heteroatom = any(molecule.GetAtomWithIdx(atom_idx).GetAtomicNum() != 6 for atom_idx in ring)\n",
    "        if has_heteroatom:\n",
    "            num_heterocycles += 1\n",
    "    \n",
    "    # Return the number of heterocycles\n",
    "    return num_heterocycles\n",
    "\n",
    "def calculate_num_spiroatoms(molecule):\n",
    "    num_spiroatoms = 0\n",
    "    \n",
    "    # Iterate over the atoms in the molecule\n",
    "    for atom in molecule.GetAtoms():\n",
    "        if atom.IsInRing() and atom.GetTotalDegree() >= 3:\n",
    "            ring_count = 0\n",
    "            for bond in atom.GetBonds():\n",
    "                if bond.GetOtherAtom(atom).IsInRing():\n",
    "                    ring_count += 1\n",
    "            if ring_count >= 2:\n",
    "                num_spiroatoms += 1\n",
    "    \n",
    "    # Return the number of spiroatoms\n",
    "    return num_spiroatoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe to store the descriptors\n",
    "descriptor_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the SMILES column in your original dataframe\n",
    "for index, row in df.iterrows():\n",
    "    smiles = row['SMILES']\n",
    "    \n",
    "    # Convert the SMILES to a molecule object\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "    \n",
    "    # Calculate the descriptors\n",
    "    descriptors = {}\n",
    "    descriptors['exactmw'] = Descriptors.ExactMolWt(molecule)\n",
    "    descriptors['amw'] = Descriptors.MolWt(molecule)\n",
    "    descriptors['lipinskiHBA'] = Descriptors.NumHAcceptors(molecule)\n",
    "    descriptors['lipinskiHBD'] = Descriptors.NumHDonors(molecule)\n",
    "    descriptors['NumRotatableBonds'] = Descriptors.NumRotatableBonds(molecule)\n",
    "    descriptors['NumHBD'] = Descriptors.NumHDonors(molecule)\n",
    "    descriptors['NumHBA'] = Descriptors.NumHAcceptors(molecule)\n",
    "    descriptors['NumHeavyAtoms'] = Descriptors.HeavyAtomCount(molecule)\n",
    "    descriptors['NumHeteroatoms'] = Descriptors.NumHeteroatoms(molecule)\n",
    "    descriptors['NumAmideBonds'] = calculate_num_amide_bonds(molecule)\n",
    "    descriptors['FractionCSP3'] = Descriptors.FractionCSP3(molecule)\n",
    "    descriptors['NumRings'] = Descriptors.RingCount(molecule)\n",
    "    descriptors['NumAromaticRings'] = Descriptors.NumAromaticRings(molecule)\n",
    "    descriptors['NumAliphaticRings'] = Descriptors.NumAliphaticRings(molecule)\n",
    "    descriptors['NumSaturatedRings'] = Descriptors.NumSaturatedRings(molecule)\n",
    "    descriptors['NumHeterocycles'] = calculate_num_heterocycles(molecule)\n",
    "    descriptors['NumAromaticHeterocycles'] = Descriptors.NumAromaticHeterocycles(molecule)\n",
    "    descriptors['NumSaturatedHeterocycles'] = Descriptors.NumSaturatedHeterocycles(molecule)\n",
    "    descriptors['NumAliphaticHeterocycles'] = Descriptors.NumAliphaticHeterocycles(molecule)\n",
    "    descriptors['NumSpiroAtoms'] = calculate_num_spiroatoms(molecule)\n",
    "    descriptors['NumBridgeheadAtoms'] = rdMolDescriptors.CalcNumBridgeheadAtoms(molecule)\n",
    "    descriptors['NumAtomStereoCenters'] = len(tuple(EnumerateStereoisomers(molecule)))\n",
    "    descriptors['labuteASA'] = Descriptors.LabuteASA(molecule)\n",
    "    descriptors['tpsa'] = Descriptors.TPSA(molecule)\n",
    "    descriptors['CrippenClogP'] = Descriptors.MolLogP(molecule)\n",
    "    descriptors['CrippenMR'] = Descriptors.MolMR(molecule)\n",
    "    descriptors['chi0v'] = Descriptors.Chi0v(molecule)\n",
    "    descriptors['chi1v'] = Descriptors.Chi1v(molecule)\n",
    "    descriptors['chi2v'] = Descriptors.Chi2v(molecule)\n",
    "    descriptors['chi3v'] = Descriptors.Chi3v(molecule)\n",
    "    descriptors['chi4v'] = Descriptors.Chi4v(molecule)\n",
    "    descriptors['chi0n'] = Descriptors.Chi0n(molecule)\n",
    "    descriptors['chi1n'] = Descriptors.Chi1n(molecule)\n",
    "    descriptors['chi2n'] = Descriptors.Chi2n(molecule)\n",
    "    descriptors['chi3n'] = Descriptors.Chi3n(molecule)\n",
    "    descriptors['chi4n'] = Descriptors.Chi4n(molecule)\n",
    "    descriptors['hallKierAlpha'] = Descriptors.HallKierAlpha(molecule)\n",
    "    descriptors['kappa1'] = Descriptors.Kappa1(molecule)\n",
    "    descriptors['kappa2'] = Descriptors.Kappa2(molecule)\n",
    "    descriptors['kappa3'] = Descriptors.Kappa3(molecule)\n",
    "    \n",
    "    # Append the descriptors to the new dataframe\n",
    "    descriptor_df = descriptor_df.append(descriptors, ignore_index=True)\n",
    "\n",
    "# Merge the original dataframe with the descriptor dataframe\n",
    "result_df = pd.concat([df, descriptor_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_lists = descriptor_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appending descriptor properties as a 1D array to ligand dataframe\n",
    "df['lb_prop'] = list_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping unnecessary columns\n",
    "columns_to_drop = ['mol', 'graph',]  # List of column names to drop\n",
    "\n",
    "# Create a new DataFrame without the specified columns\n",
    "df_new = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('KOH_inhibitor_molecules.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating molecular representations and their labels for model\n",
    "lb_props = df_new['lb_prop'].tolist()\n",
    "labels1 = df_new['KOR Inhibitor'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turning the lists into tensors\n",
    "labels = torch.tensor(labels1, dtype=torch.float32).unsqueeze(1)\n",
    "ligands = torch.tensor(lb_props, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-5eafcf093d76>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Epoch [1/100], Loss: 21.5822\n",
      "Epoch [2/100], Loss: 21.1056\n",
      "Epoch [3/100], Loss: 8.3333\n",
      "Epoch [4/100], Loss: 65.2653\n",
      "Epoch [5/100], Loss: 21.7628\n",
      "Epoch [6/100], Loss: 15.5648\n",
      "Epoch [7/100], Loss: 21.3834\n",
      "Epoch [8/100], Loss: 25.0000\n",
      "Epoch [9/100], Loss: 8.3333\n",
      "Epoch [10/100], Loss: 8.3333\n",
      "Epoch [11/100], Loss: 16.6667\n",
      "Epoch [12/100], Loss: 33.3344\n",
      "Epoch [13/100], Loss: 41.6667\n",
      "Epoch [14/100], Loss: 23.7258\n",
      "Epoch [15/100], Loss: 25.0000\n",
      "Epoch [16/100], Loss: 16.6667\n",
      "Epoch [17/100], Loss: 33.3333\n",
      "Epoch [18/100], Loss: 48.4501\n",
      "Epoch [19/100], Loss: 32.2108\n",
      "Epoch [20/100], Loss: 16.6667\n",
      "Epoch [21/100], Loss: 8.3333\n",
      "Epoch [22/100], Loss: 24.0160\n",
      "Epoch [23/100], Loss: 0.0000\n",
      "Epoch [24/100], Loss: 58.3333\n",
      "Epoch [25/100], Loss: 16.6667\n",
      "Epoch [26/100], Loss: 33.3333\n",
      "Epoch [27/100], Loss: 8.3333\n",
      "Epoch [28/100], Loss: 15.6303\n",
      "Epoch [29/100], Loss: 16.6667\n",
      "Epoch [30/100], Loss: 33.3333\n",
      "Epoch [31/100], Loss: 33.3333\n",
      "Epoch [32/100], Loss: 16.6667\n",
      "Epoch [33/100], Loss: 33.3333\n",
      "Epoch [34/100], Loss: 33.3333\n",
      "Epoch [35/100], Loss: 33.3333\n",
      "Epoch [36/100], Loss: 33.3333\n",
      "Epoch [37/100], Loss: 16.6667\n",
      "Epoch [38/100], Loss: 33.3333\n",
      "Epoch [39/100], Loss: 16.6667\n",
      "Epoch [40/100], Loss: 25.0000\n",
      "Epoch [41/100], Loss: 8.3333\n",
      "Epoch [42/100], Loss: 25.0000\n",
      "Epoch [43/100], Loss: 33.3333\n",
      "Epoch [44/100], Loss: 25.0000\n",
      "Epoch [45/100], Loss: 8.3333\n",
      "Epoch [46/100], Loss: 16.6667\n",
      "Epoch [47/100], Loss: 33.3333\n",
      "Epoch [48/100], Loss: 16.6667\n",
      "Epoch [49/100], Loss: 16.6667\n",
      "Epoch [50/100], Loss: 0.0000\n",
      "Epoch [51/100], Loss: 16.6667\n",
      "Epoch [52/100], Loss: 50.0000\n",
      "Epoch [53/100], Loss: 25.0000\n",
      "Epoch [54/100], Loss: 8.3333\n",
      "Epoch [55/100], Loss: 25.0000\n",
      "Epoch [56/100], Loss: 0.0000\n",
      "Epoch [57/100], Loss: 25.0000\n",
      "Epoch [58/100], Loss: 16.6667\n",
      "Epoch [59/100], Loss: 8.3333\n",
      "Epoch [60/100], Loss: 16.6667\n",
      "Epoch [61/100], Loss: 25.0000\n",
      "Epoch [62/100], Loss: 16.6667\n",
      "Epoch [63/100], Loss: 16.6667\n",
      "Epoch [64/100], Loss: 8.3333\n",
      "Epoch [65/100], Loss: 16.6667\n",
      "Epoch [66/100], Loss: 16.6667\n",
      "Epoch [67/100], Loss: 16.6667\n",
      "Epoch [68/100], Loss: 41.6667\n",
      "Epoch [69/100], Loss: 16.6667\n",
      "Epoch [70/100], Loss: 0.0000\n",
      "Epoch [71/100], Loss: 16.6667\n",
      "Epoch [72/100], Loss: 25.0000\n",
      "Epoch [73/100], Loss: 25.0000\n",
      "Epoch [74/100], Loss: 33.3333\n",
      "Epoch [75/100], Loss: 33.3333\n",
      "Epoch [76/100], Loss: 25.0000\n",
      "Epoch [77/100], Loss: 16.6667\n",
      "Epoch [78/100], Loss: 8.3333\n",
      "Epoch [79/100], Loss: 16.6667\n",
      "Epoch [80/100], Loss: 0.0000\n",
      "Epoch [81/100], Loss: 25.0000\n",
      "Epoch [82/100], Loss: 25.0000\n",
      "Epoch [83/100], Loss: 33.3333\n",
      "Epoch [84/100], Loss: 16.6667\n",
      "Epoch [85/100], Loss: 33.3333\n",
      "Epoch [86/100], Loss: 25.0000\n",
      "Epoch [87/100], Loss: 25.0000\n",
      "Epoch [88/100], Loss: 25.0000\n",
      "Epoch [89/100], Loss: 25.0000\n",
      "Epoch [90/100], Loss: 33.3333\n",
      "Epoch [91/100], Loss: 41.6667\n",
      "Epoch [92/100], Loss: 50.0000\n",
      "Epoch [93/100], Loss: 33.3333\n",
      "Epoch [94/100], Loss: 33.3333\n",
      "Epoch [95/100], Loss: 8.3333\n",
      "Epoch [96/100], Loss: 25.0000\n",
      "Epoch [97/100], Loss: 16.6667\n",
      "Epoch [98/100], Loss: 8.3333\n",
      "Epoch [99/100], Loss: 16.6667\n",
      "Epoch [100/100], Loss: 8.3333\n",
      "AUROC for Fold 1: 0.4257\n",
      "Fold [2/5]\n",
      "Epoch [1/100], Loss: 0.6166\n",
      "Epoch [2/100], Loss: 0.7260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-5eafcf093d76>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.6609\n",
      "Epoch [4/100], Loss: 0.6400\n",
      "Epoch [5/100], Loss: 0.5422\n",
      "Epoch [6/100], Loss: 0.6023\n",
      "Epoch [7/100], Loss: 0.3653\n",
      "Epoch [8/100], Loss: 0.5583\n",
      "Epoch [9/100], Loss: 0.6804\n",
      "Epoch [10/100], Loss: 0.7362\n",
      "Epoch [11/100], Loss: 0.2695\n",
      "Epoch [12/100], Loss: 0.7575\n",
      "Epoch [13/100], Loss: 0.5420\n",
      "Epoch [14/100], Loss: 0.5932\n",
      "Epoch [15/100], Loss: 0.5688\n",
      "Epoch [16/100], Loss: 0.4050\n",
      "Epoch [17/100], Loss: 0.3834\n",
      "Epoch [18/100], Loss: 0.2393\n",
      "Epoch [19/100], Loss: 0.7159\n",
      "Epoch [20/100], Loss: 0.6564\n",
      "Epoch [21/100], Loss: 0.6748\n",
      "Epoch [22/100], Loss: 0.5953\n",
      "Epoch [23/100], Loss: 0.4906\n",
      "Epoch [24/100], Loss: 0.3723\n",
      "Epoch [25/100], Loss: 0.3372\n",
      "Epoch [26/100], Loss: 0.4886\n",
      "Epoch [27/100], Loss: 0.3403\n",
      "Epoch [28/100], Loss: 0.3229\n",
      "Epoch [29/100], Loss: 0.4115\n",
      "Epoch [30/100], Loss: 0.3505\n",
      "Epoch [31/100], Loss: 0.4572\n",
      "Epoch [32/100], Loss: 0.5395\n",
      "Epoch [33/100], Loss: 0.3921\n",
      "Epoch [34/100], Loss: 0.4521\n",
      "Epoch [35/100], Loss: 0.2567\n",
      "Epoch [36/100], Loss: 0.3577\n",
      "Epoch [37/100], Loss: 0.5219\n",
      "Epoch [38/100], Loss: 0.1876\n",
      "Epoch [39/100], Loss: 0.3505\n",
      "Epoch [40/100], Loss: 0.4536\n",
      "Epoch [41/100], Loss: 0.6636\n",
      "Epoch [42/100], Loss: 0.3259\n",
      "Epoch [43/100], Loss: 0.4507\n",
      "Epoch [44/100], Loss: 0.3459\n",
      "Epoch [45/100], Loss: 0.3278\n",
      "Epoch [46/100], Loss: 0.2494\n",
      "Epoch [47/100], Loss: 0.4874\n",
      "Epoch [48/100], Loss: 0.6645\n",
      "Epoch [49/100], Loss: 0.5450\n",
      "Epoch [50/100], Loss: 0.5741\n",
      "Epoch [51/100], Loss: 0.5767\n",
      "Epoch [52/100], Loss: 0.6465\n",
      "Epoch [53/100], Loss: 0.5044\n",
      "Epoch [54/100], Loss: 0.3161\n",
      "Epoch [55/100], Loss: 0.2921\n",
      "Epoch [56/100], Loss: 0.4540\n",
      "Epoch [57/100], Loss: 0.4290\n",
      "Epoch [58/100], Loss: 0.3633\n",
      "Epoch [59/100], Loss: 0.6439\n",
      "Epoch [60/100], Loss: 0.4527\n",
      "Epoch [61/100], Loss: 0.4721\n",
      "Epoch [62/100], Loss: 0.1987\n",
      "Epoch [63/100], Loss: 0.2791\n",
      "Epoch [64/100], Loss: 0.4064\n",
      "Epoch [65/100], Loss: 0.5115\n",
      "Epoch [66/100], Loss: 0.3793\n",
      "Epoch [67/100], Loss: 0.2567\n",
      "Epoch [68/100], Loss: 0.1330\n",
      "Epoch [69/100], Loss: 0.2054\n",
      "Epoch [70/100], Loss: 0.4423\n",
      "Epoch [71/100], Loss: 0.3414\n",
      "Epoch [72/100], Loss: 0.3580\n",
      "Epoch [73/100], Loss: 0.4592\n",
      "Epoch [74/100], Loss: 0.3875\n",
      "Epoch [75/100], Loss: 0.2473\n",
      "Epoch [76/100], Loss: 0.4697\n",
      "Epoch [77/100], Loss: 0.2516\n",
      "Epoch [78/100], Loss: 1.0383\n",
      "Epoch [79/100], Loss: 0.3440\n",
      "Epoch [80/100], Loss: 0.5424\n",
      "Epoch [81/100], Loss: 0.3356\n",
      "Epoch [82/100], Loss: 0.3475\n",
      "Epoch [83/100], Loss: 0.3075\n",
      "Epoch [84/100], Loss: 0.3069\n",
      "Epoch [85/100], Loss: 0.2024\n",
      "Epoch [86/100], Loss: 0.2517\n",
      "Epoch [87/100], Loss: 0.3133\n",
      "Epoch [88/100], Loss: 0.2771\n",
      "Epoch [89/100], Loss: 0.4601\n",
      "Epoch [90/100], Loss: 0.4306\n",
      "Epoch [91/100], Loss: 0.3262\n",
      "Epoch [92/100], Loss: 0.5752\n",
      "Epoch [93/100], Loss: 0.5424\n",
      "Epoch [94/100], Loss: 0.3553\n",
      "Epoch [95/100], Loss: 0.1340\n",
      "Epoch [96/100], Loss: 0.2007\n",
      "Epoch [97/100], Loss: 0.3975\n",
      "Epoch [98/100], Loss: 0.2880\n",
      "Epoch [99/100], Loss: 0.4411\n",
      "Epoch [100/100], Loss: 0.5740\n",
      "AUROC for Fold 2: 0.8642\n",
      "Fold [3/5]\n",
      "Epoch [1/100], Loss: 0.4132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-5eafcf093d76>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/100], Loss: 0.5659\n",
      "Epoch [3/100], Loss: 0.3631\n",
      "Epoch [4/100], Loss: 0.5900\n",
      "Epoch [5/100], Loss: 0.4720\n",
      "Epoch [6/100], Loss: 0.5924\n",
      "Epoch [7/100], Loss: 0.3669\n",
      "Epoch [8/100], Loss: 0.4680\n",
      "Epoch [9/100], Loss: 0.6414\n",
      "Epoch [10/100], Loss: 0.2676\n",
      "Epoch [11/100], Loss: 0.3341\n",
      "Epoch [12/100], Loss: 0.6548\n",
      "Epoch [13/100], Loss: 0.3943\n",
      "Epoch [14/100], Loss: 0.3710\n",
      "Epoch [15/100], Loss: 0.2738\n",
      "Epoch [16/100], Loss: 0.4740\n",
      "Epoch [17/100], Loss: 0.4653\n",
      "Epoch [18/100], Loss: 0.3588\n",
      "Epoch [19/100], Loss: 0.4319\n",
      "Epoch [20/100], Loss: 0.3255\n",
      "Epoch [21/100], Loss: 0.4307\n",
      "Epoch [22/100], Loss: 0.2112\n",
      "Epoch [23/100], Loss: 0.6178\n",
      "Epoch [24/100], Loss: 0.5899\n",
      "Epoch [25/100], Loss: 0.2324\n",
      "Epoch [26/100], Loss: 0.3228\n",
      "Epoch [27/100], Loss: 0.6470\n",
      "Epoch [28/100], Loss: 0.3282\n",
      "Epoch [29/100], Loss: 0.2920\n",
      "Epoch [30/100], Loss: 0.4651\n",
      "Epoch [31/100], Loss: 0.5876\n",
      "Epoch [32/100], Loss: 0.4851\n",
      "Epoch [33/100], Loss: 0.4477\n",
      "Epoch [34/100], Loss: 0.3098\n",
      "Epoch [35/100], Loss: 0.5086\n",
      "Epoch [36/100], Loss: 0.9335\n",
      "Epoch [37/100], Loss: 0.3710\n",
      "Epoch [38/100], Loss: 0.3442\n",
      "Epoch [39/100], Loss: 0.3339\n",
      "Epoch [40/100], Loss: 0.3219\n",
      "Epoch [41/100], Loss: 0.5939\n",
      "Epoch [42/100], Loss: 0.2929\n",
      "Epoch [43/100], Loss: 0.4171\n",
      "Epoch [44/100], Loss: 0.5071\n",
      "Epoch [45/100], Loss: 0.6739\n",
      "Epoch [46/100], Loss: 0.2593\n",
      "Epoch [47/100], Loss: 0.5592\n",
      "Epoch [48/100], Loss: 0.5253\n",
      "Epoch [49/100], Loss: 0.1624\n",
      "Epoch [50/100], Loss: 0.4814\n",
      "Epoch [51/100], Loss: 0.3409\n",
      "Epoch [52/100], Loss: 0.2875\n",
      "Epoch [53/100], Loss: 0.4501\n",
      "Epoch [54/100], Loss: 0.3004\n",
      "Epoch [55/100], Loss: 0.2413\n",
      "Epoch [56/100], Loss: 0.4307\n",
      "Epoch [57/100], Loss: 0.1953\n",
      "Epoch [58/100], Loss: 0.7511\n",
      "Epoch [59/100], Loss: 0.1857\n",
      "Epoch [60/100], Loss: 0.3814\n",
      "Epoch [61/100], Loss: 0.4017\n",
      "Epoch [62/100], Loss: 0.4006\n",
      "Epoch [63/100], Loss: 0.6072\n",
      "Epoch [64/100], Loss: 0.2561\n",
      "Epoch [65/100], Loss: 0.3139\n",
      "Epoch [66/100], Loss: 0.4201\n",
      "Epoch [67/100], Loss: 0.5891\n",
      "Epoch [68/100], Loss: 0.3215\n",
      "Epoch [69/100], Loss: 0.4126\n",
      "Epoch [70/100], Loss: 0.2846\n",
      "Epoch [71/100], Loss: 0.3550\n",
      "Epoch [72/100], Loss: 0.1090\n",
      "Epoch [73/100], Loss: 0.6031\n",
      "Epoch [74/100], Loss: 0.4724\n",
      "Epoch [75/100], Loss: 0.2994\n",
      "Epoch [76/100], Loss: 0.4436\n",
      "Epoch [77/100], Loss: 0.1015\n",
      "Epoch [78/100], Loss: 0.4272\n",
      "Epoch [79/100], Loss: 0.4815\n",
      "Epoch [80/100], Loss: 0.3793\n",
      "Epoch [81/100], Loss: 0.4111\n",
      "Epoch [82/100], Loss: 0.1381\n",
      "Epoch [83/100], Loss: 0.3348\n",
      "Epoch [84/100], Loss: 0.2375\n",
      "Epoch [85/100], Loss: 0.2746\n",
      "Epoch [86/100], Loss: 0.4536\n",
      "Epoch [87/100], Loss: 0.2958\n",
      "Epoch [88/100], Loss: 0.3880\n",
      "Epoch [89/100], Loss: 0.3306\n",
      "Epoch [90/100], Loss: 0.4799\n",
      "Epoch [91/100], Loss: 0.5659\n",
      "Epoch [92/100], Loss: 0.5267\n",
      "Epoch [93/100], Loss: 0.3492\n",
      "Epoch [94/100], Loss: 0.4737\n",
      "Epoch [95/100], Loss: 0.3143\n",
      "Epoch [96/100], Loss: 0.5568\n",
      "Epoch [97/100], Loss: 0.4526\n",
      "Epoch [98/100], Loss: 0.3842\n",
      "Epoch [99/100], Loss: 0.7045\n",
      "Epoch [100/100], Loss: 0.2714\n",
      "AUROC for Fold 3: 0.8232\n",
      "Fold [4/5]\n",
      "Epoch [1/100], Loss: 0.5514\n",
      "Epoch [2/100], Loss: 1.1547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-5eafcf093d76>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/100], Loss: 0.5600\n",
      "Epoch [4/100], Loss: 0.4316\n",
      "Epoch [5/100], Loss: 0.4144\n",
      "Epoch [6/100], Loss: 0.6433\n",
      "Epoch [7/100], Loss: 0.4366\n",
      "Epoch [8/100], Loss: 0.4924\n",
      "Epoch [9/100], Loss: 0.5968\n",
      "Epoch [10/100], Loss: 0.4919\n",
      "Epoch [11/100], Loss: 0.5345\n",
      "Epoch [12/100], Loss: 0.6025\n",
      "Epoch [13/100], Loss: 8.1507\n",
      "Epoch [14/100], Loss: 0.5173\n",
      "Epoch [15/100], Loss: 0.5248\n",
      "Epoch [16/100], Loss: 0.4509\n",
      "Epoch [17/100], Loss: 0.3367\n",
      "Epoch [18/100], Loss: 0.7217\n",
      "Epoch [19/100], Loss: 0.4764\n",
      "Epoch [20/100], Loss: 0.3875\n",
      "Epoch [21/100], Loss: 0.4435\n",
      "Epoch [22/100], Loss: 0.5297\n",
      "Epoch [23/100], Loss: 0.6581\n",
      "Epoch [24/100], Loss: 0.5573\n",
      "Epoch [25/100], Loss: 0.7265\n",
      "Epoch [26/100], Loss: 0.4429\n",
      "Epoch [27/100], Loss: 0.4288\n",
      "Epoch [28/100], Loss: 0.5822\n",
      "Epoch [29/100], Loss: 0.4065\n",
      "Epoch [30/100], Loss: 0.8901\n",
      "Epoch [31/100], Loss: 0.7752\n",
      "Epoch [32/100], Loss: 0.7194\n",
      "Epoch [33/100], Loss: 0.8210\n",
      "Epoch [34/100], Loss: 0.3880\n",
      "Epoch [35/100], Loss: 0.5131\n",
      "Epoch [36/100], Loss: 0.3485\n",
      "Epoch [37/100], Loss: 0.4143\n",
      "Epoch [38/100], Loss: 0.4950\n",
      "Epoch [39/100], Loss: 0.2805\n",
      "Epoch [40/100], Loss: 0.8242\n",
      "Epoch [41/100], Loss: 0.3104\n",
      "Epoch [42/100], Loss: 0.2706\n",
      "Epoch [43/100], Loss: 0.3649\n",
      "Epoch [44/100], Loss: 0.2997\n",
      "Epoch [45/100], Loss: 0.5224\n",
      "Epoch [46/100], Loss: 0.6498\n",
      "Epoch [47/100], Loss: 0.7477\n",
      "Epoch [48/100], Loss: 0.7646\n",
      "Epoch [49/100], Loss: 0.3780\n",
      "Epoch [50/100], Loss: 0.6185\n",
      "Epoch [51/100], Loss: 0.5130\n",
      "Epoch [52/100], Loss: 0.2186\n",
      "Epoch [53/100], Loss: 0.2229\n",
      "Epoch [54/100], Loss: 0.3736\n",
      "Epoch [55/100], Loss: 0.5413\n",
      "Epoch [56/100], Loss: 0.3473\n",
      "Epoch [57/100], Loss: 0.5351\n",
      "Epoch [58/100], Loss: 0.4307\n",
      "Epoch [59/100], Loss: 0.2697\n",
      "Epoch [60/100], Loss: 0.3812\n",
      "Epoch [61/100], Loss: 0.4779\n",
      "Epoch [62/100], Loss: 0.5613\n",
      "Epoch [63/100], Loss: 8.4284\n",
      "Epoch [64/100], Loss: 0.3632\n",
      "Epoch [65/100], Loss: 0.4374\n",
      "Epoch [66/100], Loss: 0.1574\n",
      "Epoch [67/100], Loss: 0.5223\n",
      "Epoch [68/100], Loss: 0.3717\n",
      "Epoch [69/100], Loss: 0.6616\n",
      "Epoch [70/100], Loss: 0.3940\n",
      "Epoch [71/100], Loss: 0.6900\n",
      "Epoch [72/100], Loss: 0.9208\n",
      "Epoch [73/100], Loss: 0.3644\n",
      "Epoch [74/100], Loss: 0.5246\n",
      "Epoch [75/100], Loss: 0.4339\n",
      "Epoch [76/100], Loss: 0.2080\n",
      "Epoch [77/100], Loss: 0.3182\n",
      "Epoch [78/100], Loss: 0.6995\n",
      "Epoch [79/100], Loss: 0.6856\n",
      "Epoch [80/100], Loss: 0.5909\n",
      "Epoch [81/100], Loss: 0.3414\n",
      "Epoch [82/100], Loss: 0.6494\n",
      "Epoch [83/100], Loss: 0.6406\n",
      "Epoch [84/100], Loss: 0.2397\n",
      "Epoch [85/100], Loss: 0.4790\n",
      "Epoch [86/100], Loss: 0.7246\n",
      "Epoch [87/100], Loss: 0.2428\n",
      "Epoch [88/100], Loss: 0.1606\n",
      "Epoch [89/100], Loss: 0.4753\n",
      "Epoch [90/100], Loss: 0.4838\n",
      "Epoch [91/100], Loss: 0.3987\n",
      "Epoch [92/100], Loss: 0.3745\n",
      "Epoch [93/100], Loss: 0.6427\n",
      "Epoch [94/100], Loss: 0.3703\n",
      "Epoch [95/100], Loss: 0.1745\n",
      "Epoch [96/100], Loss: 0.7028\n",
      "Epoch [97/100], Loss: 0.4160\n",
      "Epoch [98/100], Loss: 0.4035\n",
      "Epoch [99/100], Loss: 0.3904\n",
      "Epoch [100/100], Loss: 8.1390\n",
      "AUROC for Fold 4: 0.8428\n",
      "Fold [5/5]\n",
      "Epoch [1/100], Loss: 0.4527\n",
      "Epoch [2/100], Loss: 0.8339\n",
      "Epoch [3/100], Loss: 0.3877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-5eafcf093d76>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
      "<ipython-input-14-5eafcf093d76>:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor(labels, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/100], Loss: 0.4981\n",
      "Epoch [5/100], Loss: 0.4463\n",
      "Epoch [6/100], Loss: 0.3664\n",
      "Epoch [7/100], Loss: 0.4114\n",
      "Epoch [8/100], Loss: 0.4136\n",
      "Epoch [9/100], Loss: 0.4609\n",
      "Epoch [10/100], Loss: 0.2205\n",
      "Epoch [11/100], Loss: 0.2817\n",
      "Epoch [12/100], Loss: 0.2439\n",
      "Epoch [13/100], Loss: 0.4218\n",
      "Epoch [14/100], Loss: 0.4276\n",
      "Epoch [15/100], Loss: 0.3502\n",
      "Epoch [16/100], Loss: 0.3310\n",
      "Epoch [17/100], Loss: 0.3345\n",
      "Epoch [18/100], Loss: 0.4089\n",
      "Epoch [19/100], Loss: 0.6301\n",
      "Epoch [20/100], Loss: 0.8561\n",
      "Epoch [21/100], Loss: 0.6750\n",
      "Epoch [22/100], Loss: 0.4921\n",
      "Epoch [23/100], Loss: 0.3317\n",
      "Epoch [24/100], Loss: 0.3604\n",
      "Epoch [25/100], Loss: 0.5519\n",
      "Epoch [26/100], Loss: 0.4589\n",
      "Epoch [27/100], Loss: 0.4356\n",
      "Epoch [28/100], Loss: 0.2973\n",
      "Epoch [29/100], Loss: 0.4149\n",
      "Epoch [30/100], Loss: 0.3244\n",
      "Epoch [31/100], Loss: 0.2803\n",
      "Epoch [32/100], Loss: 0.7039\n",
      "Epoch [33/100], Loss: 0.3557\n",
      "Epoch [34/100], Loss: 0.4207\n",
      "Epoch [35/100], Loss: 0.2998\n",
      "Epoch [36/100], Loss: 0.4219\n",
      "Epoch [37/100], Loss: 0.3017\n",
      "Epoch [38/100], Loss: 0.2988\n",
      "Epoch [39/100], Loss: 0.3829\n",
      "Epoch [40/100], Loss: 0.4344\n",
      "Epoch [41/100], Loss: 0.3240\n",
      "Epoch [42/100], Loss: 0.3729\n",
      "Epoch [43/100], Loss: 0.5585\n",
      "Epoch [44/100], Loss: 0.4071\n",
      "Epoch [45/100], Loss: 0.3946\n",
      "Epoch [46/100], Loss: 0.5601\n",
      "Epoch [47/100], Loss: 0.2181\n",
      "Epoch [48/100], Loss: 0.6866\n",
      "Epoch [49/100], Loss: 0.5346\n",
      "Epoch [50/100], Loss: 0.4265\n",
      "Epoch [51/100], Loss: 0.5289\n",
      "Epoch [52/100], Loss: 0.2923\n",
      "Epoch [53/100], Loss: 0.4821\n",
      "Epoch [54/100], Loss: 0.2702\n",
      "Epoch [55/100], Loss: 0.2616\n",
      "Epoch [56/100], Loss: 0.2605\n",
      "Epoch [57/100], Loss: 0.5200\n",
      "Epoch [58/100], Loss: 0.4154\n",
      "Epoch [59/100], Loss: 0.3723\n",
      "Epoch [60/100], Loss: 0.4271\n",
      "Epoch [61/100], Loss: 0.6102\n",
      "Epoch [62/100], Loss: 0.2439\n",
      "Epoch [63/100], Loss: 0.3469\n",
      "Epoch [64/100], Loss: 0.5437\n",
      "Epoch [65/100], Loss: 0.2012\n",
      "Epoch [66/100], Loss: 0.3783\n",
      "Epoch [67/100], Loss: 0.8269\n",
      "Epoch [68/100], Loss: 0.2561\n",
      "Epoch [69/100], Loss: 0.4107\n",
      "Epoch [70/100], Loss: 0.3721\n",
      "Epoch [71/100], Loss: 0.2831\n",
      "Epoch [72/100], Loss: 0.2438\n",
      "Epoch [73/100], Loss: 0.1444\n",
      "Epoch [74/100], Loss: 0.4726\n",
      "Epoch [75/100], Loss: 0.5455\n",
      "Epoch [76/100], Loss: 0.2953\n",
      "Epoch [77/100], Loss: 0.5386\n",
      "Epoch [78/100], Loss: 0.3346\n",
      "Epoch [79/100], Loss: 0.3754\n",
      "Epoch [80/100], Loss: 0.3778\n",
      "Epoch [81/100], Loss: 0.1301\n",
      "Epoch [82/100], Loss: 0.3647\n",
      "Epoch [83/100], Loss: 0.4506\n",
      "Epoch [84/100], Loss: 0.2856\n",
      "Epoch [85/100], Loss: 0.3763\n",
      "Epoch [86/100], Loss: 0.3057\n",
      "Epoch [87/100], Loss: 0.6535\n",
      "Epoch [88/100], Loss: 0.2576\n",
      "Epoch [89/100], Loss: 0.7479\n",
      "Epoch [90/100], Loss: 0.3458\n",
      "Epoch [91/100], Loss: 0.3581\n",
      "Epoch [92/100], Loss: 0.3272\n",
      "Epoch [93/100], Loss: 0.2300\n",
      "Epoch [94/100], Loss: 0.4476\n",
      "Epoch [95/100], Loss: 0.4578\n",
      "Epoch [96/100], Loss: 0.7976\n",
      "Epoch [97/100], Loss: 0.3267\n",
      "Epoch [98/100], Loss: 0.4882\n",
      "Epoch [99/100], Loss: 0.5236\n",
      "Epoch [100/100], Loss: 0.2612\n",
      "AUROC for Fold 5: 0.7941\n",
      "\n",
      "Average AUROC: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Define the neural network model\n",
    "class LigandClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(LigandClassifier, self).__init__()\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        prev_size = input_size\n",
    "        for size in hidden_sizes:\n",
    "            self.fc_layers.append(nn.Linear(prev_size, size))\n",
    "            self.fc_layers.append(nn.ReLU())\n",
    "            prev_size = size\n",
    "        self.fc_layers.append(nn.Linear(prev_size, output_size))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "        out = self.sigmoid(x)\n",
    "        return out\n",
    "\n",
    "# Define a custom dataset\n",
    "class LigandDataset(Dataset):\n",
    "    def __init__(self, ligands, labels):\n",
    "        self.ligands = torch.tensor(ligands, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ligands)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ligands[idx], self.labels[idx]\n",
    "\n",
    "ligand_dataset = LigandDataset(ligands, labels)\n",
    "\n",
    "# Define the hyperparameters\n",
    "input_size = len(ligand_dataset[0][0])\n",
    "hidden_sizes = [128, 64, 32]\n",
    "output_size = 1\n",
    "learning_rate = 0.00007\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "num_folds = 5\n",
    "\n",
    "# Create the neural network model\n",
    "model = LigandClassifier(input_size, hidden_sizes, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "fold_aurocs = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(skf.split(ligand_dataset.ligands, ligand_dataset.labels)):\n",
    "    print(f\"Fold [{fold+1}/{num_folds}]\")\n",
    "\n",
    "    # Create train and validation datasets for the current fold\n",
    "    train_dataset = LigandDataset(ligand_dataset.ligands[train_indices], ligand_dataset.labels[train_indices])\n",
    "    val_dataset = LigandDataset(ligand_dataset.ligands[val_indices], ligand_dataset.labels[val_indices])\n",
    "\n",
    "    # Create train and validation data loaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize the weights using Xavier uniform initialization\n",
    "    for layer in model.fc_layers:\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        for ligands_batch, labels_batch in train_dataloader:\n",
    "            # Forward pass\n",
    "            outputs = model(ligands_batch)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Print the loss for every epoch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Calculate AUROC for the validation set\n",
    "    val_predictions = []\n",
    "    val_labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for val_ligands, val_labels in val_dataloader:\n",
    "            val_outputs = model(val_ligands)\n",
    "            val_predictions += val_outputs.flatten().tolist()\n",
    "            val_labels_list += val_labels.tolist()\n",
    "\n",
    "    fold_auroc = roc_auc_score(val_labels_list, val_predictions)\n",
    "    fold_aurocs.append(fold_auroc)\n",
    "    print(f\"AUROC for Fold {fold+1}: {fold_auroc:.4f}\")\n",
    "\n",
    "# Calculate and print the average AUROC\n",
    "average_auroc = sum(fold_aurocs) / num_folds\n",
    "print(f\"\\nAverage AUROC: {average_auroc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
